{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Langgraph with AgentNeo Integration\n",
    " This Jupyter notebook demonstrates the integration of AgentNeo, a powerful tracing and monitoring tool, with Langgraph, a graph-based approach to managing language models with an agent-based system to enhance the automation and decision-making capabilities of your application. This integration allows for comprehensive analysis and debugging of AI-powered systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Setup and Imports\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install agentneo==1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain langchain_openai langsmith pandas langchain_experimental matplotlib langgraph langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lite-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import openai\n",
    "from litellm import completion\n",
    "from typing import List, Dict, Any\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Function for the OpenAI API key if it's not already set in the environment\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " # Initialize AgentNeo Session and Tracer\n",
    " Now, let's set up our AgentNeo session and tracer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import AgentNeo and Setup Tracing\n",
    "from agentneo import AgentNeo, Tracer, Execution, launch_dashboard \n",
    "\n",
    "# Initialize AgentNeo session\n",
    "neo_session = AgentNeo(session_name=\"langGraph_example1\")\n",
    "try:\n",
    "    neo_session.create_project(project_name=\"LangGraph_with_AgentNeo2 \")\n",
    "except:\n",
    "    neo_session.connect_project(project_name=\"LangGraph_with_AgentNeo2\")\n",
    "\n",
    "# Create tracer\n",
    "tracer = Tracer(session=neo_session)\n",
    "\n",
    "tracer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " # Define Agents and Tools\n",
    "Now, let's create our AI tools using langgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a standalone PromptInstructions class\n",
    "@tracer.trace_agent(name=\"PromptInstructions\")\n",
    "class PromptInstructions(BaseModel):\n",
    "    \"\"\"Instructions on how to prompt the LLM.\"\"\"\n",
    "\n",
    "    objective: str\n",
    "    variables: List[str]\n",
    "    constraints: List[str]\n",
    "    requirements: List[str]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class for managing prompt generation\n",
    "@tracer.trace_agent(name=\"PromptGenerator\")\n",
    "class PromptGenerator:\n",
    "    def __init__(self):\n",
    "        self.template = \"\"\"Your job is to get information from a user about what type of prompt template they want to create.\n",
    "\n",
    "        You should get the following information from them:\n",
    "\n",
    "        - What the objective of the prompt is\n",
    "        - What variables will be passed into the prompt template\n",
    "        - Any constraints for what the output should NOT do\n",
    "        - Any requirements that the output MUST adhere to\n",
    "\n",
    "        If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "\n",
    "        After you are able to discern all the information, call the relevant tool.\"\"\"\n",
    "        \n",
    "        self.llm = ChatOpenAI(temperature=0.5)\n",
    "        self.llm_with_tool = self.llm.bind_tools([PromptInstructions])\n",
    "\n",
    "    @tracer.trace_tool(name=\"get message\")\n",
    "    def get_messages_info(self, messages):\n",
    "        return [SystemMessage(content=self.template)] + messages\n",
    "\n",
    "    @tracer.trace_tool(name=\"info chain\")\n",
    "    def info_chain(self, state):\n",
    "        messages = self.get_messages_info(state[\"messages\"])\n",
    "        response = self.llm_with_tool.invoke(messages)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    @tracer.trace_tool(name=\"get prompt\")\n",
    "    def get_prompt_messages(self, messages: list):\n",
    "        tool_call = None\n",
    "        other_msgs = []\n",
    "        for m in messages:\n",
    "            if isinstance(m, AIMessage) and m.tool_calls:\n",
    "                tool_call = m.tool_calls[0][\"args\"]\n",
    "            elif isinstance(m, ToolMessage):\n",
    "                continue\n",
    "            elif tool_call is not None:\n",
    "                other_msgs.append(m)\n",
    "        prompt_system = \"\"\"Based on the following requirements, write a good prompt template:\n",
    "        \n",
    "        {reqs}\"\"\"\n",
    "        return [SystemMessage(content=prompt_system.format(reqs=tool_call))] + other_msgs\n",
    "\n",
    "    @tracer.trace_tool(name=\"prompt gen chain\")\n",
    "    def prompt_gen_chain(self, state):\n",
    "        messages = self.get_prompt_messages(state[\"messages\"])\n",
    "        response = self.llm.invoke(messages)\n",
    "        return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the PromptGenerator class\n",
    "prompt_generator = PromptGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the `Literal` type from the `typing` module, which allows defining specific literal values for type hints.\n",
    "\n",
    "from typing import Literal\n",
    "# Importing END which likely represents the end or termination of a process or a node\n",
    "from langgraph.graph import END\n",
    "\n",
    "@tracer.trace_tool(name=\"get state\")\n",
    "def get_state(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:\n",
    "        return \"add_tool_message\"\n",
    "    elif not isinstance(messages[-1], HumanMessage):\n",
    "        return END\n",
    "    return \"info\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# # Importing necessary components \n",
    "managing state graphs, saving memory, handling messages, and typing annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "@tracer.trace_agent(name=\"workflow\")\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "memory = MemorySaver()\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Instantiate the PromptGenerator class\n",
    "prompt_generator = PromptGenerator()\n",
    "\n",
    "workflow.add_node(\"info\", prompt_generator.info_chain)\n",
    "workflow.add_node(\"prompt\", prompt_generator.prompt_gen_chain)\n",
    "\n",
    "@workflow.add_node\n",
    "def add_tool_message(state: State):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=\"Prompt generated!\",\n",
    "                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "workflow.add_conditional_edges(\"info\", get_state, [\"add_tool_message\", \"info\", END])\n",
    "workflow.add_edge(\"add_tool_message\", \"prompt\")\n",
    "workflow.add_edge(\"prompt\", END)\n",
    "workflow.add_edge(START, \"info\")\n",
    "graph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Displaying a PNG image of a graph generated from a LangGraph instance using Mermaid visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " # Continuously prompts the user for input\n",
    "  streams responses from the graph, and handles fallback cached responses in case of input failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% \n",
    "import uuid\n",
    "\n",
    "cached_human_responses = [\"hi!\", \"rag prompt\", \"1 rag, 2 none, 3 no, 4 no\", \"red\", \"q\"]\n",
    "cached_response_index = 0\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user = input(\"User (q/Q to quit): \")\n",
    "    except:\n",
    "        user = cached_human_responses[cached_response_index]\n",
    "        cached_response_index += 1\n",
    "    print(f\"User (q/Q to quit): {user}\")\n",
    "    \n",
    "    if user in {\"q\", \"Q\"}:\n",
    "        print(\"AI: Byebye\")\n",
    "        break\n",
    "        \n",
    "    output = None\n",
    "    for output in graph.stream(\n",
    "        {\"messages\": [HumanMessage(content=user)]}, config=config, stream_mode=\"updates\"\n",
    "    ):\n",
    "        last_message = next(iter(output.values()))[\"messages\"][-1]\n",
    "        last_message.pretty_print()\n",
    "\n",
    "    if output and \"prompt\" in output:\n",
    "        print(\"Done!\")\n",
    "\n",
    "# %% \n",
    "# Stop Tracing and Launch Dashboard\n",
    "tracer.stop()\n",
    "print(tracer.trace_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " # Metrics Evaluation\n",
    " Supported Metrics Goal Decomposition Efficiency (goal_decomposition_efficiency) Goal Fulfillment Rate (goal_fulfillment_rate) Tool Correctness Metric (tool_correctness_metric) Tool Call Success Rate Metric (tool_call_success_rate_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "# Execute metrics\n",
    "from agentneo import Execution\n",
    "exe = Execution(session=neo_session, trace_id=tracer.trace_id)  \n",
    "exe.execute(metric_list=['goal_decomposition_efficiency', 'goal_fulfillment_rate', 'tool_call_success_rate_metric'])\n",
    "metric_results = exe.get_results()\n",
    "print(metric_results) \n",
    "metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "# Launching dashboard\n",
    "neo_session.launch_dashboard()\n",
    "\n",
    "# %%q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
