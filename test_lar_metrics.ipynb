{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import os \n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from agentneo.evaluation.metrics.learning_adaptability_rate import execute_learning_adaptability_rate_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from unittest.mock import patch, MagicMock\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(\"/Users/ejaz/Desktop/hackathon/clone3/AgentNeo/key.env\")\n",
    "\n",
    "\n",
    "# Initialize OpenAI API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentneo.evaluation.metrics.learning_adaptability_rate import (\n",
    "    extract_tool_changes,\n",
    "    analyze_post_change_performance,\n",
    "    calculate_overall_lar,\n",
    "    evaluate_learning_adaptability,\n",
    "    execute_learning_adaptability_rate_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_analyze_post_change_performance (__main__.TestLearningAdaptabilityRateMetric)\n",
      "Test analysis of performance after tool changes ... ok\n",
      "test_calculate_overall_lar (__main__.TestLearningAdaptabilityRateMetric)\n",
      "Test LAR calculation with different scenarios ... ok\n",
      "test_complete_happy_path (__main__.TestLearningAdaptabilityRateMetric)\n",
      "Test complete successful execution path ... ok\n",
      "test_error_handling (__main__.TestLearningAdaptabilityRateMetric)\n",
      "Test error handling in metric execution ... ok\n",
      "test_evaluate_learning_adaptability (__main__.TestLearningAdaptabilityRateMetric)\n",
      "Test evaluation with mocked LLM responses ... ok\n",
      "test_extract_tool_changes (__main__.TestLearningAdaptabilityRateMetric)\n",
      "Test detection of tool changes ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.011s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "from unittest.mock import patch, MagicMock\n",
    "import json\n",
    "\n",
    "class TestLearningAdaptabilityRateMetric(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.basic_config = {\n",
    "            \"model\": \"gpt-4-mini\",\n",
    "            \"temperature\": 0.0\n",
    "        }\n",
    "        \n",
    "        self.sample_tool_calls = [\n",
    "            {\"name\": \"tool1\", \"start_time\": \"2024-01-01T00:00:00\", \"output\": \"success\", \"error\": None},\n",
    "            {\"name\": \"tool2\", \"start_time\": \"2024-01-01T00:01:00\", \"output\": \"success\", \"error\": None},\n",
    "            {\"name\": \"tool1\", \"start_time\": \"2024-01-01T00:02:00\", \"output\": \"success\", \"error\": None},\n",
    "        ]\n",
    "\n",
    "    def test_extract_tool_changes(self):\n",
    "        \"\"\"Test detection of tool changes\"\"\"\n",
    "        changes = extract_tool_changes(self.sample_tool_calls)\n",
    "        \n",
    "        self.assertEqual(len(changes), 2)\n",
    "        self.assertEqual(changes[0][\"added_tools\"], [\"tool2\"])\n",
    "        self.assertEqual(changes[0][\"removed_tools\"], [\"tool1\"])\n",
    "        self.assertEqual(changes[1][\"added_tools\"], [\"tool1\"])\n",
    "        self.assertEqual(changes[1][\"removed_tools\"], [\"tool2\"])\n",
    "\n",
    "    def test_analyze_post_change_performance(self):\n",
    "        \"\"\"Test analysis of performance after tool changes\"\"\"\n",
    "        change_points = [{\n",
    "            \"timestamp\": \"2024-01-01T00:01:00\",\n",
    "            \"call_index\": 1,\n",
    "            \"added_tools\": [\"tool2\"],\n",
    "            \"removed_tools\": [\"tool1\"]\n",
    "        }]\n",
    "        \n",
    "        performance = analyze_post_change_performance(self.sample_tool_calls, change_points)\n",
    "        \n",
    "        self.assertEqual(len(performance), 1)\n",
    "        self.assertEqual(performance[0][\"successful_adaptations\"], 1)\n",
    "        self.assertEqual(performance[0][\"total_attempts\"], 1)\n",
    "\n",
    "    def test_calculate_overall_lar(self):\n",
    "        \"\"\"Test LAR calculation with different scenarios\"\"\"\n",
    "        # Test perfect adaptation\n",
    "        perfect_data = [{\n",
    "            \"successful_adaptations\": 5,\n",
    "            \"total_attempts\": 5\n",
    "        }]\n",
    "        self.assertAlmostEqual(calculate_overall_lar(perfect_data), 1.0)\n",
    "        \n",
    "        # Test partial adaptation\n",
    "        partial_data = [{\n",
    "            \"successful_adaptations\": 3,\n",
    "            \"total_attempts\": 5\n",
    "        }]\n",
    "        self.assertTrue(0 < calculate_overall_lar(partial_data) < 1)\n",
    "        \n",
    "        # Test no attempts\n",
    "        no_attempts_data = [{\n",
    "            \"successful_adaptations\": 0,\n",
    "            \"total_attempts\": 0\n",
    "        }]\n",
    "        self.assertEqual(calculate_overall_lar(no_attempts_data), 0.0)\n",
    "\n",
    "    @patch('litellm.completion')\n",
    "    def test_evaluate_learning_adaptability(self, mock_completion):\n",
    "        \"\"\"Test evaluation with mocked LLM responses\"\"\"\n",
    "        mock_response = MagicMock()\n",
    "        mock_response.choices = [\n",
    "            MagicMock(message=MagicMock(content=json.dumps({\n",
    "                \"score\": 0.8,\n",
    "                \"explanation\": \"Good adaptation pattern\",\n",
    "                \"observations\": [\"Quick recovery after changes\"],\n",
    "                \"recommendations\": [\"Continue monitoring\"]\n",
    "            })))\n",
    "        ]\n",
    "        mock_completion.return_value = mock_response\n",
    "\n",
    "        performance_data = [{\n",
    "            \"successful_adaptations\": 4,\n",
    "            \"total_attempts\": 5,\n",
    "            \"change\": {\n",
    "                \"timestamp\": \"2024-01-01T00:00:00\",\n",
    "                \"added_tools\": [\"tool2\"],\n",
    "                \"removed_tools\": [\"tool1\"]\n",
    "            }\n",
    "        }]\n",
    "\n",
    "        result = evaluate_learning_adaptability(performance_data, self.basic_config, 0.8)\n",
    "        \n",
    "        self.assertIn(\"score\", result)\n",
    "        self.assertIn(\"explanation\", result)\n",
    "        self.assertIn(\"observations\", result)\n",
    "        self.assertIn(\"recommendations\", result)\n",
    "\n",
    "    def test_error_handling(self):\n",
    "        \"\"\"Test error handling in metric execution\"\"\"\n",
    "        # Test with completely invalid trace\n",
    "        invalid_trace = None\n",
    "        result = execute_learning_adaptability_rate_metric(invalid_trace, self.basic_config)\n",
    "        self.assertEqual(result[\"result\"][\"score\"], 0.0)\n",
    "        self.assertTrue(\n",
    "            \"Error executing metric\" in result[\"result\"][\"reason\"] or \n",
    "            \"No LLM calls found\" in result[\"result\"][\"reason\"]\n",
    "        )\n",
    "\n",
    "        # Test with missing tool_calls\n",
    "        invalid_trace = {\"invalid\": \"data\"}\n",
    "        result = execute_learning_adaptability_rate_metric(invalid_trace, self.basic_config)\n",
    "        self.assertEqual(result[\"result\"][\"score\"], 0.0)\n",
    "        self.assertIn(\"No LLM calls found\", result[\"result\"][\"reason\"])\n",
    "\n",
    "        # Test with empty tool_calls\n",
    "        empty_trace = {\"tool_calls\": []}\n",
    "        result = execute_learning_adaptability_rate_metric(empty_trace, self.basic_config)\n",
    "        self.assertEqual(result[\"result\"][\"score\"], 0.0)\n",
    "        self.assertIn(\"No LLM calls found\", result[\"result\"][\"reason\"])\n",
    "\n",
    "    def test_complete_happy_path(self):\n",
    "        \"\"\"Test complete successful execution path\"\"\"\n",
    "        trace = {\n",
    "            \"tool_calls\": [\n",
    "                {\"name\": \"tool1\", \"start_time\": \"2024-01-01T00:00:00\", \"output\": \"success\", \"error\": None},\n",
    "                {\"name\": \"tool2\", \"start_time\": \"2024-01-01T00:01:00\", \"output\": \"success\", \"error\": None},\n",
    "                {\"name\": \"tool2\", \"start_time\": \"2024-01-01T00:02:00\", \"output\": \"success\", \"error\": None}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with patch('litellm.completion') as mock_completion:\n",
    "            # Mock for evaluate_learning_adaptability\n",
    "            eval_response = MagicMock()\n",
    "            eval_response.choices = [\n",
    "                MagicMock(message=MagicMock(content=json.dumps({\n",
    "                    \"score\": 0.8,\n",
    "                    \"explanation\": \"Good adaptation pattern\",\n",
    "                    \"observations\": [\"Quick recovery\"],\n",
    "                    \"recommendations\": [\"Monitor consistently\"]\n",
    "                })))\n",
    "            ]\n",
    "            \n",
    "            # Mock for generate_reason\n",
    "            reason_response = MagicMock()\n",
    "            reason_response.choices = [\n",
    "                MagicMock(message=MagicMock(content=\"Agent showed good adaptation capabilities\"))\n",
    "            ]\n",
    "            \n",
    "            mock_completion.side_effect = [eval_response, reason_response]\n",
    "            \n",
    "            result = execute_learning_adaptability_rate_metric(trace, self.basic_config)\n",
    "            \n",
    "            self.assertIn(\"score\", result[\"result\"])\n",
    "            self.assertIn(\"detailed_evaluation\", result[\"result\"])\n",
    "            self.assertIn(\"change_points\", result[\"result\"])\n",
    "            self.assertIn(\"performance_data\", result[\"result\"])\n",
    "            self.assertIn(\"reason\", result[\"result\"])\n",
    "            self.assertGreater(result[\"result\"][\"score\"], 0.0)\n",
    "\n",
    "def run_tests():\n",
    "    \"\"\"\n",
    "    Run all tests and return the result\n",
    "    This function is safe to use in both notebook and regular Python environments\n",
    "    \"\"\"\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(TestLearningAdaptabilityRateMetric)\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    return runner.run(suite)\n",
    "\n",
    "# For notebook usage\n",
    "if __name__ == '__main__':\n",
    "    run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
